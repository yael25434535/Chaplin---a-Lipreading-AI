# -*- coding: utf-8 -*-
"""modelsProofOfConcept.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16ZCjaNz2y3xkGzmIOUlWByYxdV1x5GMD

<h1><center> Visual Speech Recognition || Yael Sasson </center></h1>

## ğŸ“¦ Importing Libraries
"""

!pip install numpy==1.26.4
!pip install --upgrade evaluate jiwer
#!pip instal==gdown

import os
import cv2
import tensorflow as tf
import numpy as np
from typing import List
from matplotlib import pyplot as plt
import imageio
import gdown
import json

url = 'https://drive.google.com/uc?id=17GBUYiaZGdIldzCVK1e_b4H43xTxj1U5'
output = 'ml_project.zip'
gdown.download(url, output, quiet=True)
gdown.extractall('ml_project.zip')

"""<div dir="rtl">

 **`vocab`** ××›×™×œ ×¨×©×™××” ×©×œ ×›×œ ×”×ª×•×•×™× ×”××¤×©×¨×™×™× ×©×™×›×•×œ×™× ×œ×”×™×××¨ ×¢×œ ×™×“×™ ×”×“×•×‘×¨×™×  (××•×ª×™×•×ª ×× ×’×œ×™×ª, ×¡×™×× ×™× ××¡×•×™××™× ×•××¡×¤×¨×™×).  
×œ××—×¨ ××›×Ÿ, **`char_to_num`** × ×¢×©×” ×‘×¢×–×¨×ª `StringLookup` ×©×œ Keras ×©×××™×¨×” ×›×œ ×ª×• ×œ××¡×¤×¨ ×œ×¤×™ ×”×¡×“×¨ ×”××•×¤×™×¢ ×‘-`vocab`.  
**`num_to_char`** ××‘×¦×¢ ××ª ×”×¤×¢×•×œ×” ×”×”×¤×•×›×” â€“ ×××™×¨ ××¡×¤×¨×™× ×—×–×¨×” ×œ××•×ª×™×•×ª.  


</div>

"""

vocab = list("abcdefghijklmnopqrstuvwxyz'?!123456789 ")

char_to_num = tf.keras.layers.StringLookup(vocabulary=vocab, oov_token="")
num_to_char = tf.keras.layers.StringLookup(vocabulary=char_to_num.get_vocabulary(), oov_token="", invert=True)

print("The vocabulary is:", char_to_num.get_vocabulary())
print("size:", char_to_num.vocabulary_size())

"""<div dir="rtl">

×¤×•× ×§×¦×™×” ×œ×¢×™×‘×•×“ ×”××©×¤×˜×™× ×©× ×××¨×• ×‘×›×œ ×¡×¨×˜×•×Ÿ

×”×¤×•× ×§×¦×™×” ×§×•×¨××ª ××ª ×§×•×‘×¥ txt, ×¢×•×‘×¨×ª ×¢×œ ×›×œ ×©×•×¨×”, ×•××—×œ×¦×ª ××× ×” ××ª ×”××™×œ×™× ×ª×•×š ×”×ª×¢×œ××•×ª ××”××•×¤×¢×™× ×©×œ `'sil'` ××•  `'sp'`   
×‘×›×œ ×©×•×¨×”, × ×‘×“×§×ª ×”××™×œ×” ×”×©×œ×™×©×™×ª, ×•×× ×”×™× ×œ× `'sil'`, ×”×™× ××ª×•×•×¡×¤×ª ×œ×¨×©×™××” ×¢× ×¨×•×•×— ×œ×¤× ×™×”.  
×œ××—×¨ ××›×Ÿ, ×”×¨×©×™××” ××•××¨×ª ×œ×˜× ×–×•×¨ ×©×œ TensorFlow, ×•××•×¢×‘×¨×ª ×“×¨×š `char_to_num` ×œ×”××¨×ª ×”××•×ª×™×•×ª ×œ××¡×¤×¨×™×.  
×œ×‘×¡×•×£, ×”×ª×•×¦××” ××—×–×™×¨×” ××ª ×”×˜×•×§× ×™×, ×”××™×œ×™×, ×©×—×•×œ×¦×• ×›××¡×¤×¨×™×  

</div>

"""

def extract_alignment(align_path):
    with open(align_path, 'r') as f:
        lines = f.readlines()
    tokens = []
    for line in lines:
        line = line.split()
        if line[2] not in ['sil', 'sp']:
          tokens = [*tokens,' ',line[2]]

    # Convert to TensorFlow Tensor
    token_tensor = tf.strings.unicode_split(tokens, input_encoding='UTF-8')
    return char_to_num(tf.reshape(token_tensor, (-1)))[1:]  # Remove the first element (always 'sil')

"""
<div dir="rtl">

××§×‘×œ×ª  path, ×•×‘×¢×–×¨×ª×• ××•×¦××ª ××ª ×”paths ×©×œ ×”×¡×¨×˜×•×Ÿ ×•×©×œ ×”××©×¤×˜ ×”××ª××™× ×œ×•.  
×œ××—×¨ ××›×Ÿ, ××©×ª××©×ª ×‘×¤×•× ×§×¦×™×” ×©××¢×‘×“×ª ××ª ×”×¡×¨×˜×•×Ÿ ×œ×”××¨×ª×• ×œ×¤×¨×™×™××™×, ×•×‘×¤×•× ×§×¦×™×” ×©××¢×‘×“×ª ××ª ×§×•×‘×¦×™ ×”×˜×§×¡×˜ ×›×“×™ ×œ×—×œ×¥ ××ª ×”××™×œ×™× ×•××—×–×™×¨×” ×”×Ÿ ××ª ×”×¤×¨×™×™××™× ×•×”×Ÿ ××ª ×”alignments  

</div>

"""

def load_data(path):
    path = bytes.decode(path.numpy())
    file_name = path.split('/')[-1].split('.')[0]

    # Load preprocessed frames
    #frames_path = os.path.join('ml_project','model1','processed_100x50', f'{file_name}.npy')
    frames_path = os.path.join('ml_project','model2','processed_70x40', f'{file_name}.npy')

    frames = tf.convert_to_tensor(np.load(frames_path), dtype=tf.float32)

    # Load alignment
    alignment_path = os.path.join('ml_project','align',f'{file_name}.align')
    alignments =  extract_alignment(alignment_path)

    return frames, alignments

"""##ğŸ—„ï¸Data Pipeline

<div dir="rtl">

× ×›×™×Ÿ ××ª ×”×“××˜×”.  
×ª×—×™×œ×”, × ×™×¦×•×¨ ×¨×©×™××” ×©×œ ×›×œ ×§×•×‘×¦×™ ×”×¡×¨×˜×•× ×™×, × ×¢×¨×‘×‘ (shuffle) ××•×ª×, ×•× ××™×¨ ×›×œ `path` ×œ×¤×¨×™×™××™× ×•×œ××©×¤×˜ ×©× ×××¨.  
× ×©×ª××© ×‘-`padded_batch` ×›×š ×©:  

- ×¤×¨×™×™××™× ×™×¨×•×¤×“×• ×œ××•×¨×š ××§×¡×™××œ×™ ×©×œ **75**.  
- ×™×™×¦×•×’ ×”××™×œ×™× ×™×¨×•×¤×“ ×œ××•×¨×š ××§×¡×™××œ×™ ×©×œ **40**.  

×œ×‘×¡×•×£, × ×‘×¦×¢ **prefetch** ×œ×™×™×¢×•×œ ×”×ª×”×œ×™×š, ×•× ×—×œ×§ ××ª ×”× ×ª×•× ×™× ×›×š ×©-1800 ×“×•×’×××•×ª ×™×©××©×• ×œ-`train`, ×•×”×©××¨ ×œ-`test`, ×‘×××¦×¢×•×ª ×”×©×™×˜×•×ª `take` ×•-`skip`.  

</div>
"""

def mappable_function(path):
    result = tf.py_function(load_data, [path], (tf.float32, tf.int64))
    return result

'''
data1 =  tf.data.Dataset.list_files('./ml_project/align/*.align')
data1 = data1.shuffle(500, reshuffle_each_iteration=False)
data1 = data1.map(mappable_function)
data1 = data1.padded_batch(2, padded_shapes=([75,None,None, None],[40]))
data1 = data1.prefetch(tf.data.AUTOTUNE)
train1 = data1.take(450)
test1 = data1.skip(450)
'''

data2 = tf.data.Dataset.list_files('./ml_project/align/*.align')
data2 = data2.shuffle(500, reshuffle_each_iteration=False)
data2 = data2.map(mappable_function)
data2 = data2.padded_batch(2, padded_shapes=([75,None,None, None],[40]))
data2 = data2.prefetch(tf.data.AUTOTUNE)
train2 = data2.take(450)
test2 = data2.skip(450)

# Create directories for saving datasets
#save_dir = './saved_datasets1'
save_dir = './saved_datasets2'
os.makedirs(save_dir, exist_ok=True)

#train1.save(os.path.join(save_dir, 'train1'))
#test1.save(os.path.join(save_dir, 'test1'))
train2.save(os.path.join(save_dir, 'train2'))
test2.save(os.path.join(save_dir, 'test2'))

import shutil

# Create a ZIP archive of saved datasets
#shutil.make_archive('saved_datasets1', 'zip', 'saved_datasets1')
shutil.make_archive('saved_datasets2', 'zip', 'saved_datasets2')

'''
sample = data1.as_numpy_iterator()
val = sample.next(); val[0]
plt.imshow(val[0][0][35])
tf.strings.reduce_join([num_to_char(word) for word in val[1][0]])
'''

sample = data2.as_numpy_iterator()
val = sample.next(); val[0]
plt.imshow(val[0][0][35])
tf.strings.reduce_join([num_to_char(word) for word in val[1][0]])

"""## ğŸ› ï¸ Model Building

<div dir="rtl">

××™×™×‘× ××ª ×”×¡×¤×¨×™×•×ª ×”× ×“×¨×©×•×ª ×œ×‘× ×™×™×ª ×”××•×“×œ

</div>
"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import (Conv3D, Dense, Dropout, Bidirectional,
                                     MaxPool3D, Activation, Reshape, SpatialDropout3D,
                                     BatchNormalization, TimeDistributed, Flatten,
                                     GlobalAveragePooling3D, GRU)
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping

"""<div dir="rtl">
×”××™××“×™× ×¢×‘×•×¨ ×¡×¨×˜×•×Ÿ ×™×—×™×“
</div>
"""

#inputshape1 = data1.as_numpy_iterator().next()[0][0].shape
inputshape2 = data2.as_numpy_iterator().next()[0][0].shape

"""<div dir="rtl">

××¨×›×™×˜×§×˜×•×¨×ª ×”××•×“×œ ×©×œ×™
</div>

"""

'''
model = Sequential()
model.add(Conv3D(128, (3, 3, 3), input_shape=inputshape1, padding='same'))
model.add(Activation('relu'))
model.add(MaxPool3D((1, 2, 2)))

model.add(Conv3D(256, (3, 3, 3), padding='same'))
model.add(Activation('relu'))
model.add(MaxPool3D((1, 2, 2)))

model.add(Conv3D(75, (3, 3, 3), padding='same'))
model.add(Activation('relu'))
model.add(MaxPool3D((1, 2, 2)))

model.add(Reshape((75, -1)))
model.add(Bidirectional(GRU(128, kernel_initializer='Orthogonal', return_sequences=True)))
model.add(Dropout(.5))

model.add(Bidirectional(GRU(128, kernel_initializer='Orthogonal', return_sequences=True)))
model.add(Dropout(.5))

model.add(Dense(char_to_num.vocabulary_size()+1, kernel_initializer='he_normal', activation='softmax'))
'''

'''
model.summary()
'''

model = Sequential()
model.add(Conv3D(128, (3, 3, 3), input_shape=inputshape2, padding='same'))
model.add(Activation('relu'))
model.add(MaxPool3D((1, 2, 2)))

model.add(Conv3D(256, (3, 3, 3), padding='same'))
model.add(Activation('relu'))
model.add(MaxPool3D((1, 2, 2)))

model.add(Conv3D(75, (3, 3, 3), padding='same'))
model.add(Activation('relu'))
model.add(MaxPool3D((1, 2, 2)))

model.add(Reshape((75, -1)))
model.add(Bidirectional(GRU(128, kernel_initializer='Orthogonal', return_sequences=True)))
model.add(Dropout(.5))

model.add(Bidirectional(GRU(128, kernel_initializer='Orthogonal', return_sequences=True)))
model.add(Dropout(.5))

model.add(Dense(char_to_num.vocabulary_size()+1, kernel_initializer='he_normal', activation='softmax'))

model.summary()

"""## â­ Setup Training Options

<div dir="rtl">

×”×’×“×¨×ª ×¤×•× ×§×¦×™×™×ª ×”×”×¤×¡×“

</div>
"""

def CTCLoss(y_true, y_pred):
    batch_len = tf.cast(tf.shape(y_true)[0], dtype="int64")  # Batch size
    input_length = tf.cast(tf.shape(y_pred)[1], dtype="int64")  # Length of the input sequence (number of frames)
    label_length = tf.cast(tf.shape(y_true)[1], dtype="int64")  # Length of the target sequence (number of characters)

    input_length = input_length * tf.ones(shape=(batch_len, 1), dtype="int64")  # Expand input length for each sample in the batch
    label_length = label_length * tf.ones(shape=(batch_len, 1), dtype="int64")  # Expand label length for each sample in the batch

    loss = tf.keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)  # Compute the CTC loss
    return loss  # Return the loss value

"""<div dir="rtl">

××’×“×™×¨×”  Callback ×©××“×¤×™×¡×” ×“×•×’×××•×ª ××§×•×¨×™×•×ª ×•× ×™×‘×•×™ ×©×œ ×”××•×“×œ ×‘×¡×•×£ ×›×œ ××¤×•×§.  

</div>

"""

class ProduceExample(tf.keras.callbacks.Callback):
    def __init__(self, dataset):
        self.dataset = dataset.as_numpy_iterator()

    def on_epoch_end(self, epoch, logs=None):
        data = self.dataset.next()
        yhat = self.model.predict(data[0])  # Predict the output based on the input frames

        # Decode predictions using CTC decoding (beam search)
        decoded = tf.keras.backend.ctc_decode(yhat, [75, 75], greedy=False)[0][0].numpy()

        # Print original and predicted text for each example in the batch
        for x in range(len(yhat)):
            print('Original:', tf.strings.reduce_join(num_to_char(data[1][x])).numpy().decode('utf-8'))
            print('Prediction:', tf.strings.reduce_join(num_to_char(decoded[x])).numpy().decode('utf-8'))
            print('~' * 100)

"""<div dir="rtl">

×§×•××¤×™×œ×¦×™×” ×©×œ ×”××•×“×œ ×¢× ×”××•×¤×˜×™××™×–×˜×•×¨ `Adam` ×•×¤×•× ×§×¦×™×™×ª ×”×”×¤×¡×“ `CTCLoss`.  
×”××•×¤×˜×™××™×–×˜×•×¨ `Adam` ××©×ª××© ×‘×§×¦×‘ ×œ××™×“×” ×©×œ **0.0001**

</div>

"""

model.compile(optimizer=Adam(learning_rate=0.0001), loss=CTCLoss)

"""## ğŸš€ Model 1 Training"""

example_callback = ProduceExample(test1)

"""<div dir="rtl">

callback ×œ×©××™×¨×ª ×”××•×“×œ ×”×˜×•×‘ ×‘×™×•×ª×¨

</div>

"""

local_path = "/content/model1.keras"

model_save = ModelCheckpoint(
    local_path,
    monitor='val_loss',
    save_best_only=True,  # Save only if the model improves
    save_weights_only=False,  # Save the entire model (not just weights)
    verbose=1
)

early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=10,
    verbose=1,
    restore_best_weights=True
)

model.fit(train1, validation_data=test1, epochs=50, callbacks=[model_save, example_callback, early_stopping ])

"""<div dir="rtl">

×©××™×¨×ª ×”×”×™×¡×˜×•×¨×™×” ×‘×§×•×‘×¥ json

</div>

"""

import json
with open('historyModel1.json', 'w') as f:
    json.dump(model.history.history, f)

from google.colab import files

# After your model has been saved to local_path
files.download("/content/model1.keras")
files.download('historyModel1.json')

"""## ğŸš€ Model 2 Training"""

example_callback = ProduceExample(test2)

local_path = "/content/model2.keras"

model_save = ModelCheckpoint(
    local_path,
    monitor='val_loss',
    save_best_only=True,  # Save only if the model improves
    save_weights_only=False,  # Save the entire model (not just weights)
    verbose=1
)

early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=10,
    verbose=1,
    restore_best_weights=True
)

model.fit(train2, validation_data=test2, epochs=50, callbacks=[model_save, example_callback, early_stopping ])

import json
with open('historyModel2.json', 'w') as f:
    json.dump(model.history.history, f)

from google.colab import files

# After your model has been saved to local_path
files.download("/content/model2.keras")
files.download('historyModel2.json')

"""##val"""

def load_val_data(path):
  path = bytes.decode(path.numpy())
  file_name = path.split('/')[-1].split('.')[0]
  frames_path = os.path.join('ml_project','preVal','val_70x40', f'{file_name}.npy')
  frames = tf.convert_to_tensor(np.load(frames_path), dtype=tf.float32)
  alignment_path = os.path.join('ml_project','preVal', 'align', f'{file_name}.align')
  alignments = extract_alignment(alignment_path)
  return frames, alignments



    # Create mappable function for validation data

def val_mappable_function(path):
  result = tf.py_function(load_val_data, [path], (tf.float32, tf.int64))
  return result



    # Create validation dataset
val_data = tf.data.Dataset.list_files('./ml_project/preVal/align/*.align')
val_data = val_data.map(val_mappable_function)
val_data = val_data.padded_batch(2, padded_shapes=([75, None, None, None], [40]))
val_data = val_data.prefetch(tf.data.AUTOTUNE)

"""##ğŸ” Evaluation"""

from tensorflow.keras.models import load_model
model1 = load_model('ml_project/model1/model1.keras', compile=True, custom_objects={'CTCLoss': CTCLoss})
model2 = load_model('ml_project/model2/model2.keras', compile=True, custom_objects={'CTCLoss': CTCLoss})

train1 = tf.data.Dataset.load('/content/ml_project/model1/train1')
test1 = tf.data.Dataset.load('/content/ml_project/model1/test1')
train2 = tf.data.Dataset.load('/content/ml_project/model2/train2')
test2 = tf.data.Dataset.load('/content/ml_project/model2/test2')

with open('ml_project/model1/historyModel1.json', 'r') as f:
    historyModel1 = json.load(f)
with open('ml_project/model2/historyModel2.json', 'r') as f:
    historyModel2 = json.load(f)

"""<div dir="rtl">
×”×¦×’×ª ×’×¨×£ ×©×œ ×”×”×¤×¡×“ ×©×œ ××•×“×œ 1 ×œ××•×¨×š ×”××¤×•×§×™×.

</div>

"""

import matplotlib.pyplot as plt

plt.plot(historyModel1['loss'], label='Train Loss', color='pink')
plt.plot(historyModel1['val_loss'], label='Validation Loss', color='hotpink')

plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.title('Loss Over Epochs')

plt.show()

"""<div dir="rtl">
×”×¦×’×ª ×’×¨×£ ×©×œ ×”×”×¤×¡×“ ×©×œ ××•×“×œ 2 ×œ××•×¨×š ×”××¤×•×§×™×.

</div>

"""

import matplotlib.pyplot as plt

plt.plot(historyModel2['loss'], label='Train Loss', color='pink')
plt.plot(historyModel2['val_loss'], label='Validation Loss', color='hotpink')

plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.title('Loss Over Epochs')

plt.show()

"""<div dir="rtl">

**××—×©×‘ ××ª ×©×™×¢×•×¨ ×”×©×’×™××•×ª ×©×œ ×”××•×“×œ ×‘×××¦×¢×•×ª CER ×•-WER.**  
- **CER (Character Error Rate)** â€“ ××—×©×‘ ××ª ×©×™×¢×•×¨ ×”×©×’×™××•×ª ×‘×¨××ª ×”××•×ª×™×•×ª.  
- **WER (Word Error Rate)** â€“ ××—×©×‘ ××ª ×©×™×¢×•×¨ ×”×©×’×™××•×ª ×‘×¨××ª ×”××™×œ×™×.  

</div>

"""

from evaluate import load

# Load HuggingFace metrics once
wer_metric = load("wer")
cer_metric = load("cer")

def predict_from_video(model, video_tensor):
    yhat = model.predict(video_tensor, verbose=0)
    batch_size = yhat.shape[0]
    input_lengths = tf.constant([yhat.shape[1]] * batch_size)

    decoded, _ = tf.keras.backend.ctc_decode(yhat, input_length=input_lengths, greedy=True)
    decoded_sequences = decoded[0]

    results = []
    for seq in decoded_sequences:
        seq = tf.convert_to_tensor(seq)
        text = tf.strings.reduce_join(num_to_char(seq)).numpy().decode("utf-8")
        results.append(text)

    return results


def evaluate_wer_cer(model, dataset, max_batches=None, name=""):
    references = []
    predictions = []

    for i, batch in enumerate(dataset):


        x, y_true = batch
        y_preds = predict_from_video(model, x)

        for j in range(len(y_preds)):
            ref = tf.strings.reduce_join(num_to_char(y_true[j])).numpy().decode("utf-8")
            pred = y_preds[j]

            predictions.append(pred)
            references.append(ref)

    wer = wer_metric.compute(references=references, predictions=predictions)
    cer = cer_metric.compute(references=references, predictions=predictions)

    print(f"{name} Set:")
    print(f"WER: {wer:.2%}")
    print(f"CER: {cer:.2%}")
    return wer, cer

evaluate_wer_cer(model1, train1, num_to_char, name="Train on model 1")
evaluate_wer_cer(model1, test1, num_to_char, name="Test on model 1")
evaluate_wer_cer(model1, val_data, num_to_char, name="Validation on model 1")

evaluate_wer_cer(model2, train2, num_to_char, name="Train on model 2")
evaluate_wer_cer(model2, test2, num_to_char, name="Test on model 2")
evaluate_wer_cer(model2, val_data, num_to_char, name="Validation on model 2")

"""## ğŸš€ LightWeight architure"""

light_model = Sequential()

light_model.add(Conv3D(64, (3, 3, 3), input_shape=(75, 40, 70, 1), padding='same'))
light_model.add(Activation('relu'))
light_model.add(MaxPool3D((1, 2, 2)))

light_model.add(Conv3D(128, (3, 3, 3), padding='same'))
light_model.add(Activation('relu'))
light_model.add(MaxPool3D((1, 2, 2)))

light_model.add(Reshape((75, -1)))

light_model.add(Bidirectional(GRU(64, return_sequences=True)))
light_model.add(Dropout(0.5))

light_model.add(Dense(char_to_num.vocabulary_size() + 1, activation='softmax'))

light_model.summary()

light_model.compile(
    optimizer=Adam(learning_rate=0.0001),
    loss=CTCLoss
)
example_callback = ProduceExample(test2)

model_save = ModelCheckpoint(
    'light_model.keras',
    monitor='val_loss',
    save_best_only=True,
    verbose=1
)

early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True,
    verbose=1
)

history_light = light_model.fit(
    train2,
    validation_data=test2,
    epochs=50,
    callbacks=[model_save, example_callback, early_stopping]
)

import json
with open('historyLightModel.json', 'w') as f:
    json.dump(history_light.history, f)

with open('historyLightModel.json', 'r') as f:
    historyLightModel = json.load(f)

import matplotlib.pyplot as plt

plt.plot(historyLightModel['loss'], label='Train Loss', color='pink')
plt.plot(historyLightModel['val_loss'], label='Validation Loss', color='hotpink')

plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.title('Loss Over Epochs')

plt.show()

evaluate_wer_cer(light_model, train2, name="Train on light model")
evaluate_wer_cer(light_model, test2, name="Test on light model")
evaluate_wer_cer(light_model, val_data, name="Validation on light model")